{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread, imwrite\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# from steganogan.decoders import DenseDecoderNLayers\n",
    "# from steganogan.decoders import BasicDecoder, DenseDecoder, DenseDecoderNLayers\n",
    "# from steganogan import SteganoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.optim import LBFGS\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_params(m):\n",
    "    if type(m)==nn.Conv2d or type(m)==nn.BatchNorm2d:\n",
    "        param = m.weight\n",
    "        m.weight.data = nn.Parameter(torch.tensor(np.random.normal(0, 1, param.shape)).float())\n",
    "        \n",
    "        param = m.bias\n",
    "        m.bias.data = nn.Parameter(torch.zeros(len(param.view(-1))).float().reshape(param.shape))\n",
    "    if type(m)==nn.BatchNorm2d:\n",
    "        if \"track_running_stats\" in m.__dict__:\n",
    "            m.track_running_stats=False\n",
    "#         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class normLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(normLayer, self).__init__()\n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.shape\n",
    "        assert b == 1\n",
    "        mean = x.view(c, -1).mean(-1)\n",
    "        std = x.view(c, -1).std(-1)\n",
    "        x = x - mean.reshape([1, c, 1, 1])\n",
    "        x = x / (std + 1e-7).reshape([1,c,1,1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The BasicDecoder module takes an steganographic image and attempts to decode\n",
    "    the embedded data tensor.\n",
    "\n",
    "    Input: (N, 3, H, W)\n",
    "    Output: (N, D, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    def _conv2d(self, in_channels, out_channels):\n",
    "        return nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def _build_models(self):\n",
    "        modules = []\n",
    "\n",
    "        modules.append(self._conv2d(3, self.hidden_size))\n",
    "        modules.append(nn.LeakyReLU(inplace=True))\n",
    "        modules.append(normLayer() if self.yan_norm else nn.BatchNorm2d(self.hidden_size))\n",
    "\n",
    "        for i in range(self.layers-1):\n",
    "            modules.append(self._conv2d(self.hidden_size, self.hidden_size))\n",
    "            modules.append(nn.LeakyReLU(inplace=True))\n",
    "            modules.append(normLayer() if self.yan_norm else nn.BatchNorm2d(self.hidden_size))\n",
    "\n",
    "        modules.append(self._conv2d(self.hidden_size, self.data_depth))\n",
    "\n",
    "        self.layers = nn.Sequential(*modules)\n",
    "\n",
    "        return [self.layers]    \n",
    "\n",
    "    def __init__(self, data_depth, hidden_size, layers = 3, yan_norm=False):\n",
    "        super().__init__()\n",
    "        self.version = '1'\n",
    "        self.data_depth = data_depth\n",
    "        self.hidden_size = hidden_size\n",
    "        self.yan_norm = yan_norm\n",
    "        self.layers = layers\n",
    "\n",
    "        self._models = self._build_models()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._models[0](x)\n",
    "\n",
    "        if len(self._models) > 1:\n",
    "            x_list = [x]\n",
    "            for layer in self._models[1:]:\n",
    "                x = layer(torch.cat(x_list, dim=1))\n",
    "                x_list.append(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicDecoder(\n",
       "  (layers): Sequential(\n",
       "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.01, inplace)\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): LeakyReLU(negative_slope=0.01, inplace)\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (9): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_bits = 3\n",
    "yan_norm = False\n",
    "# models\n",
    "\n",
    "model = BasicDecoder(num_bits, hidden_size=128, layers = 3, yan_norm=yan_norm)\n",
    "model.apply(shuffle_params)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(612, 450, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load image\n",
    "image = \"/home/vk352/FaceDetection/datasets/sample/obama2.jpg\"\n",
    "image = imread(image, pilmode='RGB') \n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a bit vector\n",
    "# image = \"/home/vk352/FaceDetection/datasets/sample/obama2.jpg\"\n",
    "image = \"/home/vk352/FaceDetection/datasets/data512x512/00001.jpg\"\n",
    "image = imread(image, pilmode='RGB') / 255.0\n",
    "image = torch.FloatTensor(image).permute(2, 1, 0).unsqueeze(0)\n",
    "image = image.to('cuda')\n",
    "out = model(image)\n",
    "# image = self.decoder(image).view(-1) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.bernoulli(torch.empty(out.shape).uniform_(0, 1)).to(out.device)\n",
    "# target = torch.empty(out.shape).random_(256).to(out.device)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "criterion1 = torch.nn.L1Loss(reduction='sum')\n",
    "criterion2 = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "def get_loss(outputs, target, loss_mode):\n",
    "    if loss_mode == \"BCE\":\n",
    "        loss = criterion(outputs, target)\n",
    "    elif loss_mode == \"log\":\n",
    "        loss = -(target * 2 - 1) * outputs\n",
    "        loss = torch.nn.functional.softplus(loss)  # log(1+exp(x))\n",
    "        loss = torch.sum(loss)\n",
    "    elif loss_mode == \"hingelog\":\n",
    "        loss = -(target * 2 - 1) * outputs\n",
    "        loss = torch.nn.functional.softplus(loss)  # log(1+exp(x))\n",
    "        loss = torch.max(loss-hinge, torch.zeros(target.shape).to(target.device))\n",
    "        loss = torch.sum(loss)\n",
    "    elif loss_mode == \"L1\":\n",
    "        outputs = F.sigmoid(outputs) * 255\n",
    "        loss = criterion1(outputs, target)\n",
    "    elif loss_mode == \"L2\":\n",
    "        outputs = F.sigmoid(outputs) * 255\n",
    "        loss = criterion2(outputs, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mode = \"log\"\n",
    "hinge = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 1/50 [00:01<01:09,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0.41007359822591144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 2/50 [00:02<01:07,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0.3041369120279948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:03<01:21,  1.70s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-32ef04f69341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_image\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0madv_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/steggan/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     \u001b[0;31m# the reason we do this: in a stochastic setting,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0;31m# no use to re-evaluate that function here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                     \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0mabs_grad_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lbfgs\n",
    "\n",
    "final_err = 0\n",
    "\n",
    "steps = 1000\n",
    "eps = 0.2\n",
    "adv_image = image.clone().detach()\n",
    "max_iter = 20\n",
    "alpha = 0.5\n",
    "\n",
    "\n",
    "adv_image = image.clone().detach()\n",
    "print(\"alpha:\", alpha)\n",
    "error = []\n",
    "\n",
    "for i in trange(steps // max_iter):\n",
    "    adv_image.requires_grad = True\n",
    "    optimizer = LBFGS([adv_image], lr=alpha, max_iter=max_iter)\n",
    "\n",
    "    def closure():\n",
    "        outputs = model(adv_image)\n",
    "        loss = get_loss(outputs, target, loss_mode)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    delta = torch.clamp(adv_image - image, min=-eps, max=eps)\n",
    "    adv_image = torch.clamp(image + delta, min=0, max=1)\n",
    "    adv_image = torch.clamp(adv_image*255, 0, 255).int().float()/255.\n",
    "    adv_image = adv_image.detach()\n",
    "\n",
    "    if loss_mode in [\"L1\", \"L2\"]:\n",
    "        err = len(torch.nonzero(torch.abs(F.sigmoid(model(adv_image)).float().view(-1)*255-target.view(-1)) > 128)) / target.numel()\n",
    "    else:\n",
    "        err = len(torch.nonzero((model(adv_image)>0).float().view(-1) != target.view(-1))) / target.numel()\n",
    "    print(\"error\", err)\n",
    "    error.append(err)\n",
    "\n",
    "final_err = error[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 10))\n",
    "# # plt.plot(range(1000), np.array(bceacc) * 100, label=\"SGD\")\n",
    "# # for lr, err in final_err:\n",
    "# plt.plot(np.arange(1, steps // max_iter + 1) * max_iter, np.array(err[:steps // max_iter]) * 100, label=f\"LBFGS lr {lr}\")\n",
    "# plt.legend()\n",
    "# plt.ylabel(\"Error Rate (%)\")\n",
    "# plt.xlabel(\"iterations\")\n",
    "# plt.title(f\"{loss_mode} loss, {num_bits} bits, yan_norm {yan_norm}\")\n",
    "# # plt.ylim(0, 0.01)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steggan",
   "language": "python",
   "name": "steggan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
