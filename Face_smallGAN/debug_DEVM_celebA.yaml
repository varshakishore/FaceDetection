# Copyright (C) 2017 NVIDIA Corporation.  All rights reserved.
# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).

# Copyright (C) 2017 NVIDIA Corporation.  All rights reserved.
# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).

# logger options
image_save_iter: 5000         # How often do you want to save output images during training
image_display_iter: 100       # How often do you want to display output images during training
display_size: 32              # How many images do you want to display each time
snapshot_save_iter: 5000     # How often do you want to save trained models
log_iter: 100                   # How often do you want to log the training stats
datasetinfo: celebA
a2b: 1
linear_dim: 2
MoreCinfo: 0
self_training: 0
preserved: 0

# optimization options
max_iter: 1000000             # maximum number of training iterations
batch_size: 2 #128               # batch size
weight_decay: 0.0001          # weight decay
beta1: 0.5                    # Adam parameter
beta2: 0.999                  # Adam parameter
init: orthogonal              # initialization [gaussian/kaiming/xavier/orthogonal]
lr: 0.0002                    # initial learning rate
lr_policy: step               # learning rate scheduler
step_size: 100000              # how often to decay learning rate
gamma: 0.5                    # how much to decay learning rate
gan_w: 1                      # weight of adversarial loss
cls_w: 0.1                      # weight of classification loss
cls_w_c: 1                      # weight of classification loss
censor_w: 0                      # weight of classification loss
recon_x_cyc_w: 10              # weight of explicit style augmented cycle consistency loss
recon_x_w: 10              # weight of explicit style augmented cycle consistency loss
recon_cf_x_w: 10          # weight of prime
vgg_w: 0                      # weight of domain-invariant perceptual loss
lambda: 0                     # weight of gradient penalty

# model options
gen:
  dim: 64                     # number of filters in the bottommost layer
  input_dim: 3
  mlp_dim: 256                # number of filters in MLP
  style_dim: 8                # length of style code
  linear_dim: 2
  activ: relu                 # activation function [relu/lrelu/prelu/selu/tanh]
  n_downsample: 2             # number of downsampling layers in content encoder
  n_res: 4                    # number of residual blocks in content encoder/decoder
  pad_type: zero              # padding type [zero/reflect]
  norm: bn                    # normalization layer [none/bn/in/ln]
  new_size: 128
  #DEMM: 1
  fixed_step: 0
dis:
  type: global                # type of discriminator (global/patch)
  dim: 64                     # number of filters in the bottommost layer
  linear_dim: 2
  norm: sn                    # normalization layer [none/bn/in/ln]
  activ: lrelu                # activation function [relu/lrelu/prelu/selu/tanh]
  n_layer: 4                  # number of layers in D, but will change due to code in train_boyi.py
  gan_type: lsgan             # GAN loss [lsgan/nsgan]
  num_scales: 1               # number of scales
  pad_type: zero              # padding type [zero/reflect]
  new_size: 128

# data options
num_workers: 8                              # number of data loading threads
new_size: 128 #32                               # first resize the shortest image side to this size
classifier_root: ./models/celeA_classifiers_2/    # dataset folder location